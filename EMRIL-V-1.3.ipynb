{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a822d24-e686-4171-b020-42c00c0bdb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### EMRIL V 1.3 ####################\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from skmultiflow.trees import HoeffdingTreeClassifier\n",
    "from time import perf_counter\n",
    "from sklearn import clone\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sys\n",
    "import scipy.stats as ss\n",
    "from scipy.stats import ks_2samp\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import statistics\n",
    "from decimal import Decimal\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn import preprocessing\n",
    "from numpy import sqrt\n",
    "import matplotlib.ticker as mticker\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import math \n",
    "from numpy.linalg import norm\n",
    "import statistics\n",
    "import random\n",
    "import os\n",
    "import warnings\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from scipy.spatial  import distance_matrix\n",
    "from scipy.spatial.distance import pdist,squareform\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.float_format', '{:.8f}'.format)\n",
    "#np.set_printoptions(suppress=True)\n",
    "#np.set_printoptions(threshold=1000000000)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def __get_coefficients(y_true, y_pred_a, y_pred_b):\n",
    "    a, b, c, d = 0, 0, 0, 0\n",
    "    for i in range(y_true.shape[0]):\n",
    "        if y_pred_a[i] == y_true[i] and y_pred_b[i] == y_true[i]:\n",
    "            a = a + 1\n",
    "        elif y_pred_a[i] != y_true[i] and y_pred_b[i] == y_true[i]:\n",
    "            b = b + 1\n",
    "        elif y_pred_a[i] == y_true[i] and y_pred_b[i] != y_true[i]:\n",
    "            c = c + 1\n",
    "        else:\n",
    "            d = d + 1\n",
    " \n",
    "    return a, b, c, d\n",
    "\n",
    "def q_statistics(y_true, y_pred_a, y_pred_b):\n",
    "    a, b, c, d = __get_coefficients(y_true, y_pred_a, y_pred_b)\n",
    "    q = float(a * d - b * c) / (a * d + b * c)\n",
    "    return q\n",
    "\n",
    "def double_fault_measure(y_true, y_pred_a, y_pred_b):\n",
    "    a, b, c, d = __get_coefficients(y_true, y_pred_a, y_pred_b)\n",
    "    df = float(d) / (a + b + c + d)\n",
    "    return df\n",
    "\n",
    "def listToString(s): \n",
    "    \n",
    "    # initialize an empty string\n",
    "    str1 = \"\" \n",
    "    \n",
    "    # traverse in the string  \n",
    "    for ele in s: \n",
    "        str1 += str(ele)+ \"-\" \n",
    "    \n",
    "    # return string  \n",
    "    return str1 \n",
    "# normalize a vector to have unit norm\n",
    "def normalize(weights):\n",
    "    # calculate l1 vector norm\n",
    "    result = norm(weights, 1)\n",
    "    # check for a vector of all zeros\n",
    "    if result == 0.0:\n",
    "        return weights\n",
    "    # return normalized vector (unit norm)\n",
    "    return weights / result\n",
    "def pltcolor(lst):\n",
    "    cols=[]\n",
    "    for l in lst:\n",
    "        if l==0:\n",
    "            cols.append('blue')\n",
    "        elif l==1:\n",
    "            cols.append('red')\n",
    "    return cols\n",
    "\n",
    "\n",
    "def detect_overlap(data_min,data_maj,n_features):\n",
    "    fishers = []\n",
    "    found = 0\n",
    "    max_f1 = 0\n",
    "    max_feature_index = 0\n",
    "    Xy_up = data_min\n",
    "    Xy_down = data_maj\n",
    "    frames = [Xy_up,Xy_down]\n",
    "    Xy_d = pd.concat(frames)\n",
    "    f_data_up_means = Xy_up.mean()\n",
    "    f_data_down_means= Xy_down.mean()\n",
    "    f_data_up_vars = Xy_up.var()\n",
    "    f_data_down_vars =Xy_down.var()\n",
    "    for i in range(n_features):\n",
    "        mean_up = f_data_up_means[i]\n",
    "        mean_down = f_data_down_means[i]\n",
    "        var_up = f_data_up_vars[i]\n",
    "        var_down = f_data_down_vars[i]\n",
    "        fisher = ((mean_up-mean_down)**2) / (var_up + var_down)\n",
    "        fishers.append([i,round(fisher,5),round(mean_up,5),round(mean_down,5)])\n",
    "        if(fisher>max_f1) :\n",
    "            max_f1 = fisher\n",
    "            max_feature_index = i\n",
    "\n",
    "    max_index= max_feature_index + 1\n",
    "    #print(\"Actual Max:\",max_f1)\n",
    "    max_f1 = 1/(1+max_f1)   # push between (0,1]\n",
    "    #print(\"Transformed Max:\",max_f1)       \n",
    "    fishers=sorted(fishers,key=lambda x: x[1], reverse=False)\n",
    "    return round(max_f1,5), fishers\n",
    "\n",
    "def update_minority_buffer(X,y,n_features):\n",
    "    global minority_buffer_x\n",
    "    global minority_buffer_y \n",
    "    global w\n",
    "    global df_buffer_x\n",
    "    global min_life_max\n",
    "    \n",
    "    decrease_life()   \n",
    "    remove_zero_life_rows()\n",
    "    for n in range (0, len(X)):\n",
    "        df_buffer_x = df_buffer_x.append(X.iloc[n])\n",
    "        df_buffer_x.iloc[-1,df_buffer_x.columns.get_loc(\"life\")] = min_life_max\n",
    "        df_buffer_x.iloc[-1,df_buffer_x.columns.get_loc(\"recall_weight\")] = 1\n",
    "        df_buffer_x.iloc[-1,df_buffer_x.columns.get_loc(\"total_weight\")] = 1\n",
    "    recalculate_total_weight()\n",
    "\n",
    "def decrease_life():\n",
    "    global df_buffer_x\n",
    "    global min_life_decay_factor    \n",
    "    df_buffer_x['life'] -= min_life_decay_factor\n",
    "   # print(df_buffer_x)\n",
    "\n",
    "def update_recall_weight(weight):\n",
    "    global df_buffer_x\n",
    "    global min_life_max\n",
    "    df_buffer_x.loc[df_buffer_x.life == min_life_max, 'recall_weight'] = weight   \n",
    "    #print(df_buffer_x)\n",
    "   # sys.exit()\n",
    "    \n",
    "def recalculate_total_weight():\n",
    "    global df_buffer_x\n",
    "    global min_life_max\n",
    "    df_buffer_x['total_weight'] = 0.5* df_buffer_x['life'] + 0.5* df_buffer_x['recall_weight']   \n",
    "    #print(df_buffer_x)\n",
    "    \n",
    "#performs random oversampling  \n",
    "def resample_from_buffer(X,y,samples_to_add):\n",
    "   # print(X)\n",
    "    global df_buffer_x\n",
    "    temp_X = X\n",
    "    temp_y = y\n",
    "    temp_y = temp_y.iloc[0:0]\n",
    "    temp_X = temp_X.iloc[0:0]\n",
    "    try:\n",
    "        to_add_for_one_element = round(samples_to_add/len(df_buffer_x),0)\n",
    "        #print(\"to_add_for_one_element:\",to_add_for_one_element)\n",
    "        min_buf_copy = df_buffer_x\n",
    "        min_buf_copy = min_buf_copy.drop(['life', 'recall_weight','total_weight'], axis=1)\n",
    "        #print(len(min_buf_copy))\n",
    "        for v in range (0,len(min_buf_copy)):\n",
    "            #print(X.iloc[[v]])\n",
    "            for c in range(0,int(to_add_for_one_element)):\n",
    "                #print(\"adding resample\")\n",
    "                temp_X = temp_X.append(min_buf_copy.iloc[[v]])\n",
    "                temp_y = temp_y.append({'class': 1}, ignore_index=True)\n",
    "                #print(len(temp_X))\n",
    "        #print(temp_X)\n",
    "        #print(\"resample from buffer--return rows\")\n",
    "        #print(temp_X)\n",
    "        #print(temp_y)\n",
    "    except:\n",
    "        a = 1 #do nothing\n",
    "    \n",
    "   # print(len(temp_X))\n",
    "    #print(len(temp_y))\n",
    "    return temp_X,temp_y\n",
    "\n",
    "def remove_zero_life_rows():\n",
    "    global df_buffer_x\n",
    "    #print(\"before:\",len(df_buffer_x))  \n",
    "    df_buffer_x = df_buffer_x[df_buffer_x['total_weight'] >= 0.50]\n",
    "    #print(\"after:\",len(df_buffer_x))\n",
    "\n",
    "\n",
    "#performs random oversampling  \n",
    "def get_weighted_buffer_data(X,y):\n",
    "    global df_buffer_x\n",
    "    temp_X = X\n",
    "    temp_y = y\n",
    "    temp_y = temp_y.iloc[0:0]\n",
    "    temp_X = temp_X.iloc[0:0]\n",
    "    min_buf_copy = df_buffer_x #.loc[df_buffer_x['total_weight'] >= 0.50] \n",
    "    min_buf_copy = min_buf_copy.drop(['life', 'recall_weight','total_weight'], axis=1)    \n",
    "    for v in range (0,len(min_buf_copy)):\n",
    "            temp_X = temp_X.append(min_buf_copy.iloc[[v]])\n",
    "            temp_y = temp_y.append({'class': 1}, ignore_index=True)    \n",
    "    return temp_X,temp_y\n",
    "\n",
    "#code for fixing clusters in the data\n",
    "#user class_to_process=1 for minority space -- creates equal size clusters.\n",
    "#for majority, set class_to_process=0, performs sampling within clusters without making them equal size.\n",
    "def fix_disjuncts(X,y,samples_to_add,class_to_process,DontAddToBuffer,n_features):\n",
    "    global disjuncts\n",
    "    global output_folder\n",
    "    global disjuncts_threshold\n",
    "    global arr_ord_cols\n",
    "    global minority_buffer_x\n",
    "    global minority_buffer_y\n",
    "    global w\n",
    "    global df_buffer_x\n",
    "    global disable_buffer\n",
    "    #if minority is less than 5% in the batch \n",
    "    #then use minority buffer to do get data.    \n",
    "    min_min_count = int(w*0.05) #we need atleast 5% samples for clustering\n",
    "    \n",
    "    #If minority, see if requires samples from buffer\n",
    "    if(disable_buffer == False):\n",
    "        if(class_to_process == 1):\n",
    "            if(DontAddToBuffer == False):\n",
    "                update_minority_buffer(X,y,n_features)\n",
    "                #print(\"buffer updated !\")\n",
    "\n",
    "            if(len(X) < min_min_count):\n",
    "                #print(\"**********************using buffer or resample from buffer*********************\")\n",
    "                if(len(df_buffer_x) < min_min_count):\n",
    "                    return resample_from_buffer(X,y,samples_to_add)\n",
    "                else:\n",
    "                    X,y = get_weighted_buffer_data(X,y)\n",
    "\n",
    "\n",
    "\n",
    "    X = X.reset_index(drop=True)\n",
    "    y = y.reset_index(drop=True)\n",
    "    \n",
    "    if(len(X) < 10):\n",
    "        return X,y\n",
    "    \n",
    "    data_scaled = X\n",
    "    n_samples, n_features = X.shape\n",
    "    try:\n",
    "        data_scaled = preprocessing.scale(X)\n",
    "    except:\n",
    "        return X,y\n",
    "    linked = linkage(data_scaled, 'average')\n",
    "    last = linked[-10:, 2]\n",
    "    last_rev = last[::-1]\n",
    "    idxs = np.arange(1, len(last) + 1)\n",
    "    accele = np.diff(last, 2)  # 2nd derivative of the distances\n",
    "    accele_rev = accele[::-1]\n",
    "    k = accele_rev.argmax() + 2  # if idx 0 is the max of this we want 2 clusters\n",
    "\n",
    "    model = AgglomerativeClustering(affinity='euclidean', compute_full_tree='auto', connectivity=None, linkage='average', memory=None, n_clusters=k)\n",
    "    y_hc=model.fit_predict(X)\n",
    "    aggloclust=model.fit(X)\n",
    "    (unique, counts) = np.unique(aggloclust.labels_, return_counts=True)\n",
    "    frequencies = np.asarray((unique, counts)).T\n",
    "    frequencies = frequencies[np.argsort(frequencies[:, 1])[::-1]]\n",
    "    arr_cumsum = np.cumsum(frequencies,axis=0)\n",
    "    cumsum = arr_cumsum[:,1]\n",
    "    arr_sum_cumsum = np.sum(frequencies,axis=0)\n",
    "    arr_clusters = []\n",
    "    sum_cumsum = arr_sum_cumsum[1]\n",
    "    for i in range(len(arr_cumsum)):\n",
    "        val = cumsum[i]\n",
    "        arr_clusters.append(frequencies[i,0])\n",
    "\n",
    "    \n",
    "    arr_y_frames  = []\n",
    "    arr_x_frames  = []\n",
    "    cluster1_count =0\n",
    "    df_x1 = []\n",
    "    df_y1 = []\n",
    "    \n",
    "    disjuncts = len(arr_clusters)\n",
    "    if(disjuncts<disjuncts_threshold):\n",
    "        return X,y\n",
    "    \n",
    "    valid_clusters =0\n",
    "    for i in range(len(arr_clusters)):\n",
    "        arr_indexes = ((np.where(y_hc == int(arr_clusters[i]))))\n",
    "        df_y = y.loc[y.index[arr_indexes]]\n",
    "        if(len(df_y) > 2):\n",
    "            valid_clusters += 1\n",
    "    if(valid_clusters == 0):\n",
    "        return X,y\n",
    "    \n",
    "    each_cluster_addition = int(np.round(samples_to_add / valid_clusters ))  \n",
    "    for i in range(len(arr_clusters)):\n",
    "            arr_indexes = ((np.where(y_hc == int(arr_clusters[i]))))\n",
    "            df_x = X.loc[X.index[arr_indexes]]\n",
    "            df_y = y.loc[y.index[arr_indexes]]\n",
    "            \n",
    "            \n",
    "            if(len(df_x) <3):\n",
    "                continue\n",
    "            \n",
    "            to_add = 0\n",
    "            if(i==0):\n",
    "                cluster1_count = len(df_x)\n",
    "                to_add = each_cluster_addition\n",
    "            else:\n",
    "                to_add = each_cluster_addition\n",
    "                \n",
    "                #code to make equal size clusters in minority\n",
    "                if(class_to_process == 1):\n",
    "                    current_cluster_count = len(df_x)\n",
    "                    to_add += (cluster1_count-current_cluster_count)\n",
    "                   \n",
    "            df_cov = df_x.cov()\n",
    "            df_mean = df_x.mean(axis=0)\n",
    "            try:\n",
    "                new_samples = np.random.multivariate_normal(df_mean, df_cov,to_add)\n",
    "            except:\n",
    "                return X,y\n",
    "            \n",
    "            cols = []\n",
    "            for f in range(1,n_features+1):\n",
    "                cols.append(str(f))\n",
    "            df_new_samples = pd.DataFrame(new_samples,columns=cols)\n",
    "            df_x = pd.concat([df_x.reset_index(drop=True),df_new_samples.reset_index(drop=True)],axis=0)\n",
    "            df_x = df_x.reset_index(drop=True)\n",
    "            if(len(arr_ord_cols)>0):\n",
    "                for h in range (0,len(arr_ord_cols)):\n",
    "                    ix = arr_ord_cols[h]\n",
    "                    df_x[str(ix)] = round(df_x[str(ix)]) \n",
    "            for j in range(0,to_add):\n",
    "                df_y = df_y.append(pd.Series([class_to_process],index=['class']),ignore_index = True) \n",
    "            \n",
    "            df_y = df_y.reset_index(drop=True)            \n",
    "            arr_x_frames.append(df_x)\n",
    "            arr_y_frames.append(df_y)        \n",
    "    \n",
    "    np.set_printoptions(threshold=np.inf)            \n",
    "    modified_X = pd.concat(arr_x_frames)\n",
    "    modified_Y = pd.concat(arr_y_frames)\n",
    "    \n",
    "    modified_X = modified_X.reset_index(drop=True)\n",
    "    modified_Y = modified_Y.reset_index(drop=True)\n",
    "    #print(\"**********</Fix Disjuncts End>************\")\n",
    "    #print(modified_X.shape)\n",
    "    return modified_X,modified_Y    \n",
    "\n",
    "\n",
    "def icrc(X_maj,X_min,y_maj,y_min,n_features,overlap_val,fullFit,ups,downs):\n",
    "    global f1_overlap_value\n",
    "    global overlap_threshold_f1\n",
    "    global imbalance_ratio_threshold\n",
    "    ##################### Update Imbalance status of current window\n",
    "    is_imbalance = False\n",
    "    ir = 0\n",
    "    if(ups ==0 or downs ==0):\n",
    "        ir = imbalance_ratio_threshold + 1\n",
    "    elif(ups > downs):\n",
    "        ir = ups/downs\n",
    "    elif(ups < downs):\n",
    "        ir = downs/ups\n",
    "    elif(ups == downs):\n",
    "        ir = 1\n",
    "\n",
    "    if(ir>=imbalance_ratio_threshold):\n",
    "        is_imbalance = True\n",
    "    ########################################\n",
    "\n",
    "    #is_imbalance = False\n",
    "    if(is_imbalance == False):\n",
    "        frames = [X_min,X_maj]\n",
    "        modified_X = pd.concat(frames)\n",
    "        frames = [y_min,y_maj]\n",
    "        modified_Y = pd.concat(frames).replace('UP',1).replace('DOWN',0) \n",
    "        return modified_X.to_numpy(),modified_Y.to_numpy().reshape(-1)\n",
    "\n",
    "    if(is_imbalance==True):\n",
    "        is_overlap = False\n",
    "        fitted_already = False\n",
    "        modified_Xmin,modified_Ymin = fix_disjuncts(X_min,y_min,(len(X_maj)-len(X_min)),1,fullFit,n_features)\n",
    "        modified_Xmaj= X_maj\n",
    "        modified_Ymaj = y_maj\n",
    "        f1_overlap_value = overlap_val\n",
    "        if (f1_overlap_value > overlap_threshold_f1): \n",
    "            is_overlap =  True\n",
    "        if is_overlap:\n",
    "            modified_Xmaj = modified_Xmaj.reset_index(drop=True)\n",
    "            modified_Xmin = modified_Xmin.reset_index(drop=True)\n",
    "            idx_maj = len(modified_Xmaj)\n",
    "            \n",
    "            all_data = pd.concat([modified_Xmaj,modified_Xmin])\n",
    "            a = pdist(all_data,metric='euclidean')\n",
    "            t_l = len(all_data)\n",
    "            d_1 = linkage(a)\n",
    "            \n",
    "            d_2 = pd.DataFrame(d_1,columns=['i1','i2','i3','i4'])\n",
    "            filtered_values = np.where( ((d_2[\"i1\"] < idx_maj) & (d_2[\"i2\"] > idx_maj) & (d_2[\"i2\"] <t_l)  ) | ((d_2[\"i1\"] > idx_maj) & (d_2[\"i1\"] < t_l ) & (d_2[\"i2\"] < idx_maj) ) )\n",
    "            filtered =  d_2.loc[filtered_values]\n",
    "            arr_indexes = filtered.iloc[:,0].to_numpy() \n",
    "            to_remove = int(len(arr_indexes)*disjunct_threshold_perc_maj)            \n",
    "            drop_these = (arr_indexes[0:to_remove])\n",
    "            modified_Xmaj.drop(index=drop_these,inplace=True)\n",
    "            modified_Ymaj = modified_Ymaj.iloc[to_remove:]\n",
    "        if(len(modified_Xmin) > len(modified_Xmaj)):\n",
    "            modified_Xmaj,modified_Ymaj = fix_disjuncts(modified_Xmaj,modified_Ymaj,(len(modified_Xmin)-len(modified_Xmaj)),0,fullFit,n_features)\n",
    "\n",
    "        frames = [modified_Xmin,modified_Xmaj]\n",
    "        modified_X = pd.concat(frames)\n",
    "        frames = [modified_Ymin,modified_Ymaj]\n",
    "        modified_Y = pd.concat(frames).replace('UP',1).replace('DOWN',0) \n",
    "        \n",
    "    return modified_X.to_numpy(),modified_Y.to_numpy().reshape(-1)\n",
    "\n",
    "class DetectorClassifier(BaseEstimator):\n",
    "    def __init__(self, clf, classes):\n",
    "        if not hasattr(clf, \"partial_fit\"):\n",
    "            raise TypeError(\"Choose incremental classifier\")\n",
    "        self.clf = clf\n",
    "        classes = [0,1]\n",
    "        self.classes = classes\n",
    "        self.change_detected = 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        #self.clf = clone(self.clf)\n",
    "        self.clf.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def partial_fit(self,modified_X,modified_Y,fullFit):\n",
    "            if(fullFit):\n",
    "               # print(\"^^^^Full Fit^^^\")    \n",
    "                self.clf = self.clf.fit(modified_X, modified_Y, classes=self.classes)            \n",
    "            else:\n",
    "               # print(\"^^^^Partial Fit^^^\")\n",
    "                self.clf = self.clf.partial_fit(modified_X, modified_Y, classes=self.classes)\n",
    "\n",
    "            return self\n",
    "    \n",
    "    ##### Prdict Function        \n",
    "    def predict(self, X):\n",
    "        return np.asarray(self.clf.predict(X), dtype=np.int64).ravel()\n",
    "    \n",
    "    ##### Score Function\n",
    "    def score(self, X,y):\n",
    "        return self.clf.score(X,y)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self.clf.predict_proba(X) \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "  \n",
    "def prequential(window_size,Xy,X, y, clf,n_features,overlap_threshold,imbalance_ratio_threshold,disjunct_threshold_perc_maj,file_out_prefix, n_train=1):\n",
    "    \n",
    "    \"\"\"Prequential Evaluation: instances are first used to test, and then to train\n",
    "    :return the label predictions for each test instance, and the associated running time\n",
    "    \"\"\"\n",
    "    first_weights_flag = False\n",
    "    \n",
    "    arr_clf = []\n",
    "    arr_clf_temp_to_use = []\n",
    "    arr_weights = []\n",
    "    arr_recall0_cw = []\n",
    "    arr_recall1_cw = []\n",
    "    \n",
    "    a= 0.00001\n",
    "    start = timer()\n",
    "\n",
    "    global arr_elapsedtime\n",
    "    global restart_threshold\n",
    "    global arr_recall_0\n",
    "    global arr_recall_1\n",
    "    global output_folder\n",
    "    global maj_weight\n",
    "    global min_weight\n",
    "    global ens_weight\n",
    "    global ensemble_pool_size    \n",
    "    global pruned_ensemble_size\n",
    "    global rl_pruned_ids\n",
    "    global preq_Qtable\n",
    "\n",
    "    preq_Qtable = initialize_q_table(ensemble_pool_size, ensemble_pool_size)\n",
    "    arr_actual_win = []\n",
    "    arr_pred_win = []\n",
    "    arr_roc = []\n",
    "    arr_divs = []\n",
    "    ensemble_tp = 0\n",
    "    ensemble_fp =0\n",
    "    ensemble_fn = 0\n",
    "    ensemble_tn = 0\n",
    "    \n",
    "    \n",
    "    ensemble_correct =0\n",
    "    zeroClassCounter = 0\n",
    "    oneClassCounter = 0\n",
    "    current_correct_0 = 0\n",
    "    current_correct_1 = 0\n",
    "    \n",
    "    current_correct_ensemble_0 = 0\n",
    "    current_correct_ensemble_1 = 0\n",
    "    \n",
    "    predictions = \"\"    \n",
    "    row_num = y.shape[0]   \n",
    "   \n",
    "    # Split an init batch\n",
    "    X_init = X[0:n_train]\n",
    "    y_init = y[0:n_train]\n",
    "    \n",
    "    # Used for training and evaluation\n",
    "    X_train = X\n",
    "    y_train = y\n",
    "    \n",
    "    Xy_train = Xy\n",
    "    \n",
    "        \n",
    "    ensemble_preds = np.zeros(row_num)\n",
    "    current_correct_0_cw = np.zeros(ensemble_pool_size)\n",
    "    current_correct_1_cw = np.zeros(ensemble_pool_size)\n",
    "    recall_0_cw = np.zeros(ensemble_pool_size)\n",
    "    recall_1_cw = np.zeros(ensemble_pool_size)\n",
    "    \n",
    "   \n",
    "    for g in range (0,ensemble_pool_size):\n",
    "        #Create a classifier and add it to the pool\n",
    "        obj_clf = DetectorClassifier(HoeffdingTreeClassifier(), [0,1])\n",
    "        obj_clf = obj_clf.fit(X_init, y_init)\n",
    "        arr_clf.append(obj_clf)\n",
    "        arr_weights.append(1)\n",
    "    \n",
    "    \n",
    "    current_w = 0\n",
    "    window_counter = 0\n",
    "    strProcessing = \"\"\n",
    "    correct_predictions = 0\n",
    "\n",
    "    strout = \"\"\n",
    "    strgmean = \"\"\n",
    "\n",
    "    oneClassCounter_CWindow = 0\n",
    "    zeroClassCounter_CWindow = 0\n",
    "    arr_cw_ens_preds = []\n",
    "    current_correct_0_cw_est = 0\n",
    "    current_correct_1_cw_est = 0\n",
    "    window_recall_0 = 0\n",
    "    window_recall_1 = 0\n",
    "        \n",
    "    for i in range(0, row_num):\n",
    "\n",
    "        current_w += 1\n",
    "        start_time = perf_counter()\n",
    "        arr_preds = []\n",
    "        arr_temp_preds = []\n",
    "        \n",
    "        for n in range(0,len(arr_clf)):\n",
    "            clf = arr_clf[n]\n",
    "            prediction_clf = clf.predict(X_train[i, :].reshape(1, -1))\n",
    "            arr_preds.append(prediction_clf[0])\n",
    "            arr_temp_preds.append(prediction_clf[0])\n",
    "        arr_cw_ens_preds.append(arr_temp_preds)    \n",
    "        #sys.exit()\n",
    "       \n",
    "        strout += \"------------\\n\"\n",
    "        estimators_arr = []\n",
    "        for n in range(0,len(arr_clf)):\n",
    "            estimators_arr.append((\"HT-\" + str(n), arr_clf[n]))\n",
    "        \n",
    "       \n",
    "        df_estimators_arr = pd.DataFrame(arr_clf)\n",
    "        df_weights_arr = pd.DataFrame(arr_weights)        \n",
    "        df_weights_arr = df_weights_arr.sort_values(ascending=False,by=df_weights_arr.columns[0])        \n",
    "        arr_clf_temp_to_use = []\n",
    "        estimators_arr_temp = []\n",
    "        arr_weights_temp = []\n",
    "\n",
    "        for h in range(0,len(rl_pruned_ids)) :\n",
    "            try:\n",
    "                indexx = rl_pruned_ids[h].astype(int)\n",
    "            except:\n",
    "                indexx = rl_pruned_ids[h]\n",
    "\n",
    "            index_original = df_weights_arr.iloc[[h]].index.values[0]\n",
    "            arr_clf_temp_to_use.append(arr_clf[indexx])\n",
    "            estimators_arr_temp.append((\"HT-\" + str(index_original), arr_clf[indexx]))\n",
    "            arr_weights_temp.append(arr_weights[indexx])\n",
    "        \n",
    "        if(len(arr_clf_temp_to_use) == 0):\n",
    "            for h in range(0,len(arr_clf)) :               \n",
    "                index_original = df_weights_arr.iloc[[h]].index.values[0]\n",
    "                arr_clf_temp_to_use.append(arr_clf[index_original])\n",
    "                estimators_arr_temp.append((\"HT-\" + str(index_original), arr_clf[index_original]))\n",
    "                arr_weights_temp.append(arr_weights[index_original])\n",
    "       \n",
    "        pred = 0\n",
    "        try:\n",
    "            eclf = VotingClassifier(estimators = estimators_arr_temp, voting='soft') \n",
    "            eclf.estimators_ = arr_clf_temp_to_use\n",
    "            eclf.le_ = LabelEncoder().fit(np.unique([0, 1]))\n",
    "            pred_proba = eclf.predict_proba(X_train[i, :].reshape(1, -1))\n",
    "            pred = eclf.predict(X_train[i, :].reshape(1, -1))\n",
    "            arr_tempo = pred_proba[0]\n",
    "            arr_pred_win.append(arr_tempo[1])\n",
    "        except:\n",
    "            eclf = VotingClassifier(estimators = estimators_arr_temp, voting='hard') \n",
    "            eclf.estimators_ = arr_clf_temp_to_use\n",
    "            eclf.le_ = LabelEncoder().fit(np.unique([0, 1]))\n",
    "            pred = eclf.predict(X_train[i, :].reshape(1, -1))\n",
    "            if(pred==0):\n",
    "                arr_pred_win.append(0)\n",
    "            if(pred==1):\n",
    "                arr_pred_win.append(1)  \n",
    "        \n",
    "        #print(\"i:\",i)\n",
    "        ensemble_preds[i] = pred\n",
    "        #print(\"ens_size:\",len(ensemble_preds))\n",
    "        #sys.exit()\n",
    "        counter = i+1\n",
    "        lambda_0 = maj_weight#Majority\n",
    "        lambda_1 = min_weight #Minority\n",
    "        \n",
    "        if y_train[i] == 0:\n",
    "            zeroClassCounter += 1\n",
    "            zeroClassCounter_CWindow += 1\n",
    "        else: \n",
    "            oneClassCounter += 1\n",
    "            oneClassCounter_CWindow += 1\n",
    "        correct_found = 0\n",
    "\n",
    "        for n in range(0,len(arr_clf)):\n",
    "            if(y_train[i] == 0 and arr_preds[n] ==0):\n",
    "                current_correct_0_cw[n] = current_correct_0_cw[n]+1\n",
    "            if(y_train[i] == 1 and arr_preds[n] ==1):    \n",
    "                current_correct_1_cw[n] = current_correct_1_cw[n]+1\n",
    "        \n",
    "        correct_found = 0  \n",
    "        y_actual = 0\n",
    "        y_pred = 0\n",
    "        if(y_train[i] ==0 and pred ==0):\n",
    "            correct_found +=1\n",
    "            current_correct_ensemble_0 += 1\n",
    "            current_correct_0_cw_est += 1\n",
    "            ensemble_tn += 1\n",
    "            y_actual = 0\n",
    "            y_pred = 0\n",
    "        if(y_train[i] ==1 and pred == 1):\n",
    "            correct_found +=1\n",
    "            current_correct_ensemble_1 += 1\n",
    "            current_correct_1_cw_est += 1\n",
    "            ensemble_tp += 1\n",
    "            y_actual = 1\n",
    "            y_pred = 1\n",
    "        if(y_train[i] ==1 and pred ==0):\n",
    "            ensemble_fn += 1\n",
    "            y_actual = 1\n",
    "            y_pred = 0\n",
    "        if(y_train[i] ==0 and pred == 1):\n",
    "            ensemble_fp += 1\n",
    "            y_actual = 0\n",
    "            y_pred = 1\n",
    "        if(correct_found ==1):\n",
    "            ensemble_correct += 1\n",
    "            \n",
    "        arr_actual_win.append(y_actual)                   \n",
    "        \n",
    "        \n",
    "        #when the batch is ready / an incomplete batch at the end\n",
    "        if( ( current_w % window_size == 0)  or (current_w != window_size and i == (row_num - w -1)) ):\n",
    "\n",
    "            window_counter += 1\n",
    "            \n",
    "            window_recall_0 = 0\n",
    "            window_recall_1 = 0\n",
    "            if zeroClassCounter_CWindow != 0:\n",
    "                window_recall_0 =  current_correct_0_cw_est / zeroClassCounter_CWindow\n",
    "            if oneClassCounter_CWindow != 0:\n",
    "                window_recall_1 =  current_correct_1_cw_est / oneClassCounter_CWindow\n",
    "            \n",
    "            window_gmean = sqrt(window_recall_0*window_recall_1)\n",
    "            \n",
    "            \n",
    "            update_recall_weight( (lambda_0*window_recall_0) + (lambda_1*window_recall_1))\n",
    "            \n",
    "            #print(\"current window gmean:\",str(window_gmean*100))\n",
    "            arr_gmean.append(window_gmean*100)\n",
    "            arr_recall_0.append(window_recall_0*100)\n",
    "            arr_recall_1.append(window_recall_1*100)\n",
    "            \n",
    "            accuracy = (ensemble_tp+ensemble_tn)/(zeroClassCounter_CWindow+oneClassCounter_CWindow)\n",
    "\n",
    "            arr_actual_win=[]\n",
    "            arr_pred_win = []\n",
    "            arr_cw_ens_preds = []            \n",
    "            \n",
    "            for n in range(0,len(arr_clf)):                    \n",
    "                if zeroClassCounter_CWindow != 0:\n",
    "                    recall_0_cw[n] =  current_correct_0_cw[n] / zeroClassCounter_CWindow                    \n",
    "                else:\n",
    "                    recall_0_cw[n] =  1\n",
    "                    \n",
    "            for n in range(0,len(arr_clf)):\n",
    "                    if oneClassCounter_CWindow != 0:\n",
    "                        recall_1_cw[n] =  current_correct_1_cw[n] / oneClassCounter_CWindow\n",
    "                    else:\n",
    "                        recall_1_cw[n] = 1\n",
    "\n",
    "            arr_weights = [(lambda_0 * recall_0_cw[n]) + (lambda_1 * recall_1_cw[n]) for n in range(len(arr_clf))]\n",
    "\n",
    "            arr_weights = normalize(arr_weights)\n",
    "\n",
    "            oneClassCounter_CWindow = 0\n",
    "            zeroClassCounter_CWindow = 0  \n",
    "            current_correct_0_cw_est = 0\n",
    "            current_correct_1_cw_est = 0\n",
    "            ensemble_tn = 0\n",
    "            ensemble_tp = 0\n",
    "            ensemble_fp = 0\n",
    "            ensemble_fn = 0\n",
    "            \n",
    "            current_correct_0_cw = [0] * len(arr_clf)\n",
    "            current_correct_1_cw = [0] * len(arr_clf)\n",
    "            \n",
    "           \n",
    "            x_preq = X_train[i-(window_size-1):i]\n",
    "            y_preq = y_train[i-(window_size-1):i]\n",
    "\n",
    "            Xy_preq = Xy_train[i-(window_size-1):i]\n",
    "            ups =  np.count_nonzero(y_preq == 1)\n",
    "            downs = np.count_nonzero(y_preq == 0)            \n",
    "\n",
    "            arr_clf_temp = []\n",
    "\n",
    "            ####################### <Overlap Detection> ############################################\n",
    "            #Check overlaps where and send its status to partialfit\n",
    "            cols_str = []\n",
    "            for f in range(1,n_features+1):\n",
    "                cols_str.append(str(f))\n",
    "            cols_str.append('class')\n",
    "            Xy_d = pd.DataFrame(Xy_preq, columns = cols_str)\n",
    "            if(ups < downs):\n",
    "                Xy_min = Xy_d[(Xy_d['class'] == 1)]\n",
    "                Xy_maj = Xy_d[(Xy_d['class'] == 0)]\n",
    "            if(ups>downs):\n",
    "                Xy_min = Xy_d[(Xy_d['class'] == 0 )]\n",
    "                Xy_maj = Xy_d[(Xy_d['class'] == 1 )]\n",
    "\n",
    "            cols = []\n",
    "            for f in range(0,n_features):\n",
    "                cols.append(f)\n",
    "            cols_class = [n_features]\n",
    "            X_min = Xy_min[Xy_min.columns[cols]]\n",
    "            X_maj = Xy_maj[Xy_maj.columns[cols]]\n",
    "            y_min = Xy_min[Xy_min.columns[cols_class]]\n",
    "            y_maj = Xy_maj[Xy_maj.columns[cols_class]]\n",
    "            \n",
    "            frames = [X_min,X_maj]\n",
    "            modified_X = pd.concat(frames)\n",
    "            frames = [y_min,y_maj]\n",
    "            modified_Y = pd.concat(frames)\n",
    "\n",
    "            cols_num = []            \n",
    "            cols_num = list(range(n_features))\n",
    "\n",
    "            m_X = modified_X\n",
    "            m_Y = modified_Y            \n",
    "            m_X.columns = cols_num\n",
    "            m_Y.columns = [0]  \n",
    "            \n",
    "            \n",
    "            overlap_val, fishers = detect_overlap(X_min, X_maj,n_features)   \n",
    "            \n",
    "            ####################### </Drift Detection and Overlap> ############################################\n",
    "            \n",
    "            modified_x,modified_y = icrc(X_maj,X_min,y_maj,y_min,n_features,overlap_val,False,ups,downs)\n",
    "                   \n",
    "            arr_temp_clfs = []\n",
    "            ## Partial Fit the previous classifiers\n",
    "            for n in range(0,len(arr_clf)):\n",
    "                clf = arr_clf[n]\n",
    "                clf = clf.partial_fit(modified_x,modified_y,False)\n",
    "                arr_temp_clfs.append(clf)\n",
    "             \n",
    "            rl_pruned_ids = []\n",
    "            #################################### Commented out code that removes the minimum preto based member ###############\n",
    "            arr_clf = arr_temp_clfs\n",
    "            #remove one classifier if the pool has reached..\n",
    "            remove_index = 0\n",
    "            start1 = timer()\n",
    "            if(len(arr_clf) == ensemble_pool_size):\n",
    "                least_weight = arr_weights[0]\n",
    "                for q in range (1,len(arr_weights)):\n",
    "                    w_q = arr_weights[q]\n",
    "                    if(w_q < least_weight):\n",
    "                        remove_index = q\n",
    "                arr_clf = np.delete(arr_clf,remove_index)\n",
    "                arr_weights = np.delete(arr_weights,remove_index)\n",
    "            global elt_global\n",
    "            end1 = timer()\n",
    "            elt= end1-start1\n",
    "            elt_global += elt    \n",
    "            ######################### add new classifier to the pool after training it with the current window.#####################\n",
    "            obj_clf1 = DetectorClassifier(HoeffdingTreeClassifier(), [0,1])\n",
    "            clf_temp =obj_clf1.partial_fit(modified_x,modified_y,True)\n",
    " \n",
    "            arr_weights = np.append(arr_weights,0)\n",
    "            arr_clf = np.append(arr_clf,clf_temp)\n",
    "            \n",
    "            preq_Qtable = np.delete(preq_Qtable, remove_index, axis=1)\n",
    "            preq_Qtable = np.delete(preq_Qtable, remove_index, axis=0)\n",
    "                        \n",
    "            row_of_ones = np.zeros((1, preq_Qtable.shape[1]), dtype=preq_Qtable.dtype)\n",
    "            preq_Qtable = np.vstack((preq_Qtable, row_of_ones))\n",
    "\n",
    "            # Add a column containing ones for all rows\n",
    "            column_of_ones = np.zeros((preq_Qtable.shape[0], 1), dtype=preq_Qtable.dtype)\n",
    "            preq_Qtable = np.hstack((preq_Qtable, column_of_ones))\n",
    "\n",
    "            if(len(arr_clf) == ensemble_pool_size):\n",
    "\n",
    "                rl_pruned_ids,preq_Qtable = get_pruned_rl_members(arr_clf,x_preq, y_preq,window_counter,preq_Qtable)\n",
    "            ##########################################################################################################        \n",
    "            current_w = 0\n",
    "            prev_win_x = x_preq\n",
    "            prev_win_y =  y_preq\n",
    "            \n",
    "            end = timer()\n",
    "            elapsed_time = end - start\n",
    "            arr_elapsedtime.append(elapsed_time)\n",
    "            print(elapsed_time)\n",
    "            #if(window_counter % 50 == 0):\n",
    "            #    np.savetxt(output_folder + file_out_prefix+ \"-\"  +str(window_counter) +'.csv', np.c_[arr_recall_0,arr_recall_1,arr_gmean], delimiter=',',fmt='%2.5f')\n",
    "               \n",
    "    \n",
    "    np.savetxt(output_folder + file_out_prefix + '.csv',  np.c_[arr_recall_0,arr_recall_1,arr_gmean,arr_elapsedtime], delimiter=',',fmt='%1.2f')\n",
    "    \n",
    "def initialize_q_table(state_space, action_space):\n",
    "  Qtable = np.zeros((state_space, action_space))\n",
    "  return Qtable    \n",
    "    \n",
    "def take_step(ensemble,action,X_test,y_test,prev_perf):\n",
    "    global ensemble_pool_size    \n",
    "    ensemble.append(action)    \n",
    "    eclf = VotingClassifier(estimators = ensemble, voting='soft') \n",
    "    eclf.estimators_ = ensemble\n",
    "    eclf.le_ = LabelEncoder().fit(np.unique([0, 1]))\n",
    "\n",
    "    #print(y_test)\n",
    "    y_pred = np.zeros(y_test.shape)\n",
    "    \n",
    "    if(len(ensemble) == 1):\n",
    "        eclf = ensemble[0]\n",
    "        \n",
    "    y_pred = np.zeros(y_test.shape)\n",
    "    \n",
    "    zero_corrects = 0\n",
    "    zeros_count = 0\n",
    "    one_corrects = 0\n",
    "    ones_count = 0\n",
    "\n",
    "    num_zeros = np.count_nonzero(y_test == 0)\n",
    "    num_ones = np.count_nonzero(y_test == 1)\n",
    "       \n",
    "    if len(X_test.shape) == 1:\n",
    "        X_test = X_test.reshape(1, -1)  # Reshape to a 2D array with a single row\n",
    "\n",
    "    # Predict all examples in X_test\n",
    "    predictions = eclf.predict(X_test)\n",
    "\n",
    "    # Compare predictions with y_test\n",
    "    zero_corrects = np.sum((y_test == 0) & (predictions == 0))\n",
    "    one_corrects = np.sum((y_test == 1) & (predictions == 1))     \n",
    "   \n",
    "    recall_0 = zero_corrects / num_zeros\n",
    "    if(ones_count ==0):\n",
    "        recall_1 = 0\n",
    "    else:\n",
    "        recall_1 = one_corrects / num_ones\n",
    "    weight_en = recall_0*maj_weight + recall_1*min_weight\n",
    "    \n",
    "    reward = weight_en - prev_perf\n",
    "\n",
    "    new_state = action\n",
    "    done = False\n",
    "    if(len(ensemble)>ensemble_pool_size or reward > 0.99):\n",
    "        done = True\n",
    "    return new_state, reward, done, 0,weight_en\n",
    "\n",
    "def read_data(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    data = df.values\n",
    "    return data, data[:, :-1], data[:, -1]\n",
    "\n",
    "def delete_files_from_folder(folder):\n",
    "    for filename in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
    "\n",
    "def train_rl(gamma,learning_rate,min_epsilon, max_epsilon, decay_rate, max_steps, Qtable,X_test,y_test,classifiers,window_counter):\n",
    "  global rl_episodes \n",
    "  global restart_threshold\n",
    "  global rl_each_step_episodes\n",
    "  global ensemble_pool_size\n",
    "  ep = 0 \n",
    "  rl_episodes_to_use = rl_episodes\n",
    "  if(window_counter==2):\n",
    "     rl_episodes_to_use = rl_episodes\n",
    "\n",
    "  elif( window_counter % restart_threshold ==0):\n",
    "    rl_episodes_to_use = rl_episodes\n",
    "  else:\n",
    "    rl_episodes_to_use = rl_each_step_episodes\n",
    "  for episode in range(0,rl_episodes_to_use):\n",
    "    ep += 1\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "\n",
    "    running_ep_val = epsilon\n",
    "    # Reset the environment\n",
    "    state_index= random.randint(0, ensemble_pool_size-1)\n",
    "\n",
    "    state = classifiers[state_index]\n",
    "    step = 0\n",
    "    done = False\n",
    "    ensemble = []\n",
    "    \n",
    "    predictive_performance = 0.90\n",
    "    # repeat\n",
    "    for step in range(max_steps):\n",
    "      action_index = epsilon_greedy_policy(Qtable, state_index, epsilon,False)\n",
    "      if(action_index==state_index or (action_index in ensemble)):\n",
    "        continue\n",
    "      ensemble.append(state)\n",
    "\n",
    "      action = classifiers[action_index]\n",
    "\n",
    "      new_state, reward, done, info,predictive_performance = take_step(ensemble,action,X_test,y_test,predictive_performance)\n",
    "      new_state = action_index\n",
    "\n",
    "      Qtable[state_index][action_index] = Qtable[state_index][action_index] + learning_rate * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state_index][action_index])\n",
    "      # If done, finish the episode\n",
    "      if done:\n",
    "        break\n",
    "     \n",
    "      # Our state is the new state\n",
    "      state_index =new_state\n",
    "      state =  classifiers[new_state]\n",
    "  return Qtable\n",
    "\n",
    "def epsilon_greedy_policy(Qtable, state, epsilon,running_ep_val):\n",
    "\n",
    "    random_int = random.uniform(0,1)\n",
    "    if running_ep_val:\n",
    "        action = np.argmax(Qtable[state])\n",
    "    elif random_int > epsilon:\n",
    "        action = np.argmax(Qtable[state])\n",
    "    else:\n",
    "        action = random.randint(0, ensemble_pool_size-1)\n",
    "\n",
    "    return action\n",
    "\n",
    "def get_pruned_rl_members(trained_members,X_test,y_test,window_counter,QTable):\n",
    "\n",
    "    global restart_threshold\n",
    "    if(window_counter==2):\n",
    "        Qtable_ens = initialize_q_table(ensemble_pool_size,ensemble_pool_size)\n",
    "\n",
    "    elif(window_counter % restart_threshold ==0):\n",
    "        Qtable_ens = initialize_q_table(ensemble_pool_size,ensemble_pool_size)\n",
    "    else:\n",
    "        Qtable_ens = QTable\n",
    "    \n",
    "    global learning_rate\n",
    "    global decay_factor\n",
    "\n",
    "    # Environment parameters\n",
    "    max_steps = 15             \n",
    "    gamma = 0.95               \n",
    "    eval_seed = []             \n",
    "\n",
    "    # Exploration parameters\n",
    "    max_epsilon = 1.0           \n",
    "    min_epsilon = 0.05           \n",
    " \n",
    "    \n",
    "    ensemble_arr = []\n",
    "    weights_arr = []\n",
    "    Qtable = train_rl(gamma,learning_rate,min_epsilon, max_epsilon, decay_factor, max_steps, Qtable_ens,X_test,y_test,trained_members,window_counter)\n",
    "    classifiers = trained_members\n",
    "    rl_pruned_ids = []\n",
    "    ensemble = []\n",
    "    state_index= random.randint(0, ensemble_pool_size-1)\n",
    "    rl_pruned_ids.append(state_index)\n",
    "    state = classifiers[state_index]\n",
    "    ensemble.append(state)\n",
    "    step = 0\n",
    "    done = False\n",
    "    ensemble = []\n",
    "\n",
    "    \n",
    "    #max_perf\n",
    "    max_gmean = 0\n",
    "    max_index = 0\n",
    "    step_index= -1\n",
    "    # repeat\n",
    "    for step in range(max_steps):\n",
    "        step_index += 1\n",
    "        action_index = epsilon_greedy_policy(Qtable, state_index,0, True)\n",
    "\n",
    "        if(action_index==state_index or (action_index in rl_pruned_ids)):\n",
    "            continue\n",
    "        \n",
    "        rl_pruned_ids.append(action_index)\n",
    "\n",
    "        action = classifiers[action_index]\n",
    "\n",
    "        ensemble.append(action)\n",
    "\n",
    "        eclf = VotingClassifier(estimators = ensemble, voting='soft') \n",
    "        eclf.estimators_ = ensemble\n",
    "        eclf.le_ = LabelEncoder().fit(np.unique([0, 1]))\n",
    "\n",
    "        num_zeros = np.count_nonzero(y_test == 0)\n",
    "        num_ones = np.count_nonzero(y_test == 1)\n",
    "            \n",
    "        if len(X_test.shape) == 1:\n",
    "            X_test = X_test.reshape(1, -1)  # Reshape to a 2D array with a single row\n",
    "\n",
    "        # Predict all examples in X_test\n",
    "        predictions = eclf.predict(X_test)\n",
    "\n",
    "        # Compare predictions with y_test\n",
    "        zero_corrects = np.sum((y_test == 0) & (predictions == 0))\n",
    "        one_corrects = np.sum((y_test == 1) & (predictions == 1))     \n",
    "\n",
    "        recall_0 = zero_corrects / num_zeros\n",
    "        if(num_ones ==0):\n",
    "            recall_1 = 0\n",
    "        else:\n",
    "            recall_1 = one_corrects / num_ones\n",
    "        gmean = math.sqrt (recall_0 * recall_1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if(gmean > max_gmean):\n",
    "            max_index = step_index\n",
    "            max_gmean = gmean\n",
    "        \n",
    "        state_index =action_index\n",
    "        state =  classifiers[state_index]\n",
    "        done = False\n",
    "        if(len(ensemble) == pruned_ensemble_size ):\n",
    "\n",
    "            break\n",
    "\n",
    "    return rl_pruned_ids, Qtable              \n",
    "            \n",
    "#*********************************  Main ******************************** #\n",
    "\n",
    "global imbalance_threshold\n",
    "global arr_recall_0\n",
    "global arr_recall_1\n",
    "global arr_gmean\n",
    "global restart\n",
    "global restart_threshold\n",
    "global f1_overlap_value\n",
    "global maj_weight\n",
    "global min_weight\n",
    "global ens_weight\n",
    "global arr_ord_cols\n",
    "global preq_Qtable\n",
    "global restart_threshold\n",
    "global min_life_max\n",
    "global min_life_decay_factor\n",
    "global output_folder\n",
    "global stream_folder\n",
    "\n",
    "global ensemble_pool_size\n",
    "global pruned_ensemble_size\n",
    "global rl_pruned_ids\n",
    "global learning_rate\n",
    "global decay_factor\n",
    "global minority_buffer_x\n",
    "global minority_buffer_y\n",
    "minority_buffer_x = []\n",
    "minority_buffer_y = []\n",
    "global df_buffer_x\n",
    "global rl_each_step_episodes\n",
    "global arr_elapsedtime\n",
    "global elt_global\n",
    "global disable_buffer\n",
    "\n",
    "\n",
    "elt_global = 0\n",
    "df_buffer_x = pd.DataFrame()\n",
    "f1_overlap_value = 0\n",
    "\n",
    "cwd = os.getcwd()\n",
    "cwd = \"/home/temp\"\n",
    "#cwd = \"D:\\\\usman-data\"\n",
    "output_folder = cwd +  '/output/DRDVEN-V1/main/'\n",
    "stream_folder = cwd +  '/streams/EMRIL/v1-neuro/'\n",
    "\n",
    "arr_elapsedtime = []\n",
    "arr_gmean = []\n",
    "arr_recall_0 = []\n",
    "arr_recall_1 = []\n",
    "rl_pruned_ids = []\n",
    "\n",
    "ensemble_pool_size = 15 #ensemble pool size\n",
    "pruned_ensemble_size =8#pruned ensemble size\n",
    "restart_threshold = 50 #batches after q-table is emptied\n",
    "rl_episodes = 5 #on restart\n",
    "rl_each_step_episodes =1\n",
    "\n",
    "n_train = 200 #no of instances to train the model (offline)\n",
    "w =200 #batch size\n",
    "overlap_threshold_f1 = 0.0 #overlap threshold\n",
    "imbalance_ratio_threshold =1.01 #IR threshold\n",
    "disjuncts_threshold =2 #MinD Threshold\n",
    "disjunct_threshold_perc_maj = 1.0 #overlap removal threshold\n",
    "maj_weight = 0.1 #lambda_0 majority weight in ensemble\n",
    "min_weight = 0.9 #Lambda_1 minority weight in ensemble\n",
    "min_life_max = 0.99 #minoirty instance life max value\n",
    "min_life_decay_factor = 0.03 #minority instance life decay factor\n",
    "disable_buffer = False\n",
    "\n",
    "#/*If data stream contains ordinal cols, provide indices here*/\n",
    "arr_ord_cols= []\n",
    "#arr_ord_cols = [6,7,8,9,10] #1-based index  GMSC\n",
    "#arr_ord_cols = [13,14,15,16,17,18,19,20,21] #1-based index  IJCNN1\n",
    "#arr_ord_cols = [10,11,12] #1-based index  LOAN\n",
    "#arr_ord_cols = [1,2] #1-based index  Mixed\n",
    "\n",
    "#filename = \"v5_imb.csv\"\n",
    "#filename = \"v9_imb.csv\"\n",
    "#filename = \"ijcnn1-full-30oct2022.csv\"\n",
    "#filename = \"cod-rna-311022.csv\"\n",
    "#filename = \"MiniBooNE_PID_Mod.csv\"\n",
    "#filename =  \"GMSC.csv\"\n",
    "#filename = \"loan_191122.csv\"\n",
    "#filename = \"noaa.csv\"\n",
    "#filename = \"mixed50K_Static01Abr.csv\"\n",
    "\n",
    "\n",
    "f_arr = filename.split(\".\")\n",
    "\n",
    "filename = stream_folder  + filename\n",
    "\n",
    "Xy, X, y = read_data(filename)\n",
    "# Set x,y as numeric\n",
    "X = X.astype(float)\n",
    "n_samples, n_features = X.shape\n",
    "print(n_samples)\n",
    "print(n_features)\n",
    "ups =  np.count_nonzero(y == 1)\n",
    "downs = np.count_nonzero(y == 0)\n",
    "print(\"1s:\",str(ups/n_samples))\n",
    "print(\"0s:\",str(downs/n_samples))\n",
    "\n",
    "#create the minority buffer structure\n",
    "cols_str = []\n",
    "for f in range(1,n_features+1):\n",
    "    cols_str.append(str(f))\n",
    "cols_str.append(\"life\")\n",
    "cols_str.append(\"recall_weight\")\n",
    "cols_str.append(\"total_weight\")\n",
    "df_buffer_x = pd.DataFrame(columns = cols_str)\n",
    "for k in range (1,11):\n",
    "    elt_global = 0\n",
    "    decay_factor = 0.005\n",
    "    learning_rate = 0.8    \n",
    "    arr_recall_0= []\n",
    "    arr_recall_1= []\n",
    "    arr_gmean= []\n",
    "    arr_elapsedtime = []\n",
    "    df_buffer_x = pd.DataFrame(columns = cols_str)\n",
    "    \n",
    "    file_out_prefix = \"EMRILs_\" + f_arr[0] + \"_\" + str(k) \n",
    "    clfs = [DetectorClassifier(HoeffdingTreeClassifier(),[0,1])]\n",
    "    clfs_label = [\"Hoeffding Tree Classifier\"]\n",
    "    \n",
    "    for i in range(len(clfs)):\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            prequential(w,Xy,X, y, clfs[i], n_features,overlap_threshold_f1,\n",
    "                        imbalance_ratio_threshold,disjunct_threshold_perc_maj,file_out_prefix,n_train)\n",
    "sys.exit()    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f792cddf-1f2d-467e-9738-88bb32269dc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4ded8b-17f9-48ba-9dce-32fd128e5c8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf9b20a-3bbe-4a2d-8c3f-7adeccf3ad88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
